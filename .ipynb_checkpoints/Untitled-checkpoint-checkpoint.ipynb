{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b42da814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\prachi varshney\\appdata\\roaming\\python\\python311\\site-packages (from seaborn) (1.23.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.39.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\prachi varshney\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\prachi varshney\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=1.2->seaborn) (2022.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Requirement already satisfied: WordCloud in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\prachi varshney\\appdata\\roaming\\python\\python311\\site-packages (from WordCloud) (1.23.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\prachi varshney\\appdata\\roaming\\python\\python311\\site-packages (from WordCloud) (9.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from WordCloud) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->WordCloud) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->WordCloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->WordCloud) (4.39.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->WordCloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->WordCloud) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->WordCloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->WordCloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prachi varshney\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->WordCloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install seaborn\n",
    "!pip install WordCloud\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import contractions\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.collocations import TrigramAssocMeasures, TrigramCollocationFinder\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a639809",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mb_data.csv\")\n",
    "data.head(10)\n",
    "# to see how data looks and to check whether the dataset is uploaded fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a29c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.info()\n",
    "# to check how many entries are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24108c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e901eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the classes of personality types in the dataset\n",
    "_classes = data.type.unique()\n",
    "print(_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd461a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code defines a function named show_class_distribution that takes a dataset, \n",
    "# a column name representing categories, and optional parameters for plot customization,\n",
    "# then visualizes the distribution of categories using Seaborn's countplot function.\n",
    "def show_class_distribution(data, x=\"type\", figsize=(16,4), title=\"Distribution of Personality Types\", xticks_size=10, palette=\"husl\"):\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.countplot(x=x, data=data, palette=palette)\n",
    "    plt.xlabel(\"Personality Types\", size=15)\n",
    "    plt.ylabel(\"Counts\", size=15)\n",
    "    plt.xticks(size=xticks_size)\n",
    "    plt.title(title, size=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb329d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_class_distribution(data, xticks_size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543c25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This code defines a function called divide_types that adds four new columns (\"E-I\", \"N-S\", \"F-T\", \"J-P\") to a DataFrame (df). \n",
    "# def divide_types(df):\n",
    "def divide_types(df):\n",
    "    df[\"E-I\"] = \"\"\n",
    "    df[\"N-S\"] = \"\"\n",
    "    df[\"F-T\"] = \"\"\n",
    "    df[\"J-P\"] = \"\"\n",
    "    for index, row in df.iterrows():\n",
    "        row[\"E-I\"] = \"E\" if row.type[0] == \"E\" else \"I\"\n",
    "        row[\"N-S\"] = \"N\" if row.type[1] == \"N\" else \"S\"\n",
    "        row[\"F-T\"] = \"F\" if row.type[2] == \"F\" else \"T\"\n",
    "        row[\"J-P\"] = \"J\" if row.type[3] == \"J\" else \"P\"\n",
    "    return df\n",
    "\n",
    "data = divide_types(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_class_distribution(data, x=\"E-I\", title=\"Distribution of I & E\", figsize=(9,3), xticks_size=20, palette=\"icefire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b7403",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_class_distribution(data, x=\"N-S\", title=\"Distribution of N & S\", figsize=(9,3), xticks_size=20, palette=\"cubehelix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e8fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_class_distribution(data, x=\"F-T\", title=\"Distribution of F & T\", figsize=(9,3), xticks_size=20, palette=\"viridis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7579733",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_class_distribution(data, x=\"J-P\", title=\"Distribution of J & P\", figsize=(9,3), xticks_size=20, palette=\"flare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36982e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves the value located at row 7\n",
    "data.loc[7, \"posts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f86049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix_contractions:\n",
    "\n",
    "\n",
    "def fix_contractions(df, column_name = \"posts\", new_column=\"cleaned_post\"):\n",
    "    df[new_column] = df[column_name].apply(lambda x: contractions.fix(x))\n",
    "    return df\n",
    "\n",
    "data = fix_contractions(data)\n",
    "def clean_data(df, column_name = \"cleaned_post\"):\n",
    "    df[column_name] = df[column_name].apply(lambda x: x.lower())\n",
    "    df[column_name] = df[column_name].apply(lambda x: re.sub(r'@([a-zA-Z0-9_]{1,50})', '', x))\n",
    "    df[column_name] = df[column_name].apply(lambda x: re.sub(r'#([a-zA-Z0-9_]{1,50})', '', x))\n",
    "    df[column_name] = df[column_name].apply(lambda x: re.sub(r'http[s]?://\\S+', '', x))\n",
    "    df[column_name] = df[column_name].apply(lambda x: re.sub(r'[^A-Za-z]+', ' ', x))\n",
    "    df[column_name] = df[column_name].apply(lambda x: re.sub(r' +', ' ', x))\n",
    "    df[column_name] = df[column_name].apply(lambda x: \" \".join([word for word in x.split() if not len(word) <3]))\n",
    "    return df\n",
    "\n",
    "data = clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b742d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[7,\"cleaned_post\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"words_count\"] = data[\"cleaned_post\"].apply(lambda x: len(x.split()))\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_counts(df, column, xlabel):\n",
    "    fig = plt.figure()\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    df[column].plot.hist(bins=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a266a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    plot_counts(data, column=\"words_count\", xlabel=\"Words Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"char_count\"] = data[\"cleaned_post\"].apply(lambda x: len(x))\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09870f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_counts(data, column=\"char_count\", xlabel=\"Character Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1114a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a1d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c660337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent(data, stop_words, column=\"cleaned_post\", top=25):\n",
    "    df = data[column].apply(lambda x: \" \".join([word for word in x.split() if not word in stop_words]))\n",
    "    counter = Counter(\" \".join(df).split())\n",
    "    return counter.most_common(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfd7e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequents = get_most_frequent(data, stopword_list)\n",
    "most_frequents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_frequents(most_frequent_words, top=20):\n",
    "    most_frequent_df = pd.DataFrame(most_frequent_words)    \n",
    "    plt.figure(figsize=(16,4))\n",
    "    my_cmap = plt.get_cmap(\"viridis\")\n",
    "    plt.bar(x=most_frequent_df.iloc[:top, 0], height=most_frequent_df.iloc[:top, 1], color=\"slateblue\")\n",
    "    plt.xlabel(\"Words\", size=17)\n",
    "    plt.ylabel(\"Counts\", size=17)\n",
    "    plt.title(\"Most Frequent Words\", size = 20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f1bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_most_frequents(most_frequents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab0c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_wordcloud(data, stopword_list, column=\"cleaned_post\"):\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    wordcloud = WordCloud(background_color=\"black\", min_font_size=5, stopwords=stopword_list).generate(data[column].to_string())\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c48d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(data, stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b1402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sub_wordclouds(data, type_column, column, size, fig_size=(20,15)):\n",
    "    classes = data[type_column].unique()\n",
    "    fig, ax = plt.subplots(len(classes), figsize=fig_size)\n",
    "    j = 0\n",
    "    for _class in classes:\n",
    "        temp = data[data[type_column] == _class]\n",
    "        wordcloud = WordCloud(background_color=\"black\").generate(temp[column].to_string())\n",
    "        plt.subplot(*size, j+1)\n",
    "        plt.title(_class, size=25)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d2931",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sub_wordclouds(data, type_column=\"type\" , column=\"cleaned_post\", size=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa9ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sub_wordclouds(data, type_column=\"E-I\" , column=\"cleaned_post\", size=(1,2), fig_size=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec9719",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sub_wordclouds(data, type_column=\"N-S\" , column=\"cleaned_post\", size=(1,2), fig_size=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a9f2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "show_sub_wordclouds(data, type_column=\"F-T\" , column=\"cleaned_post\", size=(1,2), fig_size=(16,8))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c258cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "show_sub_wordclouds(data, type_column=\"J-P\" , column=\"cleaned_post\", size=(1,2), fig_size=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0b1e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(data, n_gram, new_column, column=\"cleaned_post\"):\n",
    "    data[\"tokenized\"]  = data[column].apply(lambda x: x.split())\n",
    "    data[\"sw_removal\"] = data[\"tokenized\"].apply(lambda x: [y for y in x if not y in stopword_list])\n",
    "    data[new_column]   = data[\"sw_removal\"].apply(lambda x: list(ngrams(x, n_gram)))\n",
    "    data.drop(columns  = [\"tokenized\", \"sw_removal\"], inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6640ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_ngrams(data, n_gram=2, new_column=\"bigrams\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b98165",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_ngrams(data, n_gram=3, new_column=\"trigrams\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1edddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_ngram(data, column, top=20):\n",
    "    temp = []\n",
    "    for index, row in data.iterrows():\n",
    "        temp += row[column]\n",
    "    most_common = Counter(temp).most_common(top)\n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1fd5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_n_grams(ngrams, title, top=20):\n",
    "    ngram_df = pd.DataFrame(ngrams)\n",
    "    ngram_df.iloc[:, 0] = ngram_df.iloc[:,0].astype(str)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.barh(y=ngram_df.iloc[:top, 0], width=ngram_df.iloc[:top, 1])\n",
    "    plt.xlabel(\"Counts\", size=17)\n",
    "    plt.ylabel(\"Pairs\", size=17)\n",
    "    plt.title(title, size = 20)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e9b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_most_common = most_common_ngram(data, \"bigrams\")\n",
    "bigrams_most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_grams(bigrams_most_common, title=\"Most Frequent Bigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b8fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trigrams_most_common = most_common_ngram(data, \"trigrams\")\n",
    "trigrams_most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a00fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_grams(trigrams_most_common, title=\"Most Frequent Trigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da13bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58435f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_stopwords(data, stopword_list, column=\"cleaned_post\"):\n",
    "    data[column] = data[column].apply(word_tokenize)\n",
    "    data[column] = data[column].apply(lambda x: [word for word in x if not word in stopword_list])\n",
    "    return data\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d554bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca56986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lemmatize(data, stopword_list, column=\"cleaned_post\"):\n",
    "    data[column] = data[column].apply(apply_lemmatization)\n",
    "    data[column] = data[column].apply(\" \".join)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db214d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f04a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_stopwords(data, stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85405569",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d011f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lemmatize(data, stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf043402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc2f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data[[\"cleaned_post\", \"E-I\", \"N-S\", \"F-T\", \"J-P\"]].copy()\n",
    "training_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48282615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dummies(data, columns=[\"E-I\", \"N-S\", \"F-T\", \"J-P\"]):\n",
    "    for column in columns:\n",
    "        temp_dummy = pd.get_dummies(data[column], prefix=\"type\")\n",
    "        data = data.join(temp_dummy)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = make_dummies(training_data)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_data[[\"cleaned_post\"]]\n",
    "y = training_data.drop(columns=[\"cleaned_post\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  this function provides a convenient way to visualize the distribution of categorical variables in a DataFrame\n",
    "def show_distribution(data, x=[\"E-I\", \"N-S\", \"F-T\",\"J-P\"], fig_size=(16,4), xticks_size=10, palette=\"husl\"):\n",
    "    fig, ax = plt.subplots(len(x), figsize=fig_size)\n",
    "    j = 0\n",
    "    for _x in x:\n",
    "        plt.subplot(1,4, j+1)\n",
    "        sns.countplot(x=_x, data=data, palette=palette)\n",
    "        plt.xticks(size=xticks_size)\n",
    "        j+=1    \n",
    "        \n",
    "show_distribution(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b2f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32cef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e77051",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_ei = y[\"type_E\"]\n",
    "y_ns = y[\"type_N\"]\n",
    "y_ft = y[\"type_F\"]\n",
    "y_jp = y[\"type_J\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951e835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_over_ei, y_over_ei = oversample.fit_resample(X, y_ei)\n",
    "X_over_ns, y_over_ns = oversample.fit_resample(X, y_ns)\n",
    "X_over_ft, y_over_ft = oversample.fit_resample(X, y_ft)\n",
    "X_over_jp, y_over_jp = oversample.fit_resample(X, y_jp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f56757",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "show_class_distribution(data=X_over_ei, x=y_over_ei, figsize=(7,3), title=\"E-I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe978f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "show_class_distribution(data=X_over_ns, x=y_over_ns, figsize=(7,3), title=\"N-S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_class_distribution(data=X_over_ft, x=y_over_ft, figsize=(7,3), title=\"F-T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081356f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_class_distribution(data=X_over_jp, x=y_over_jp, figsize=(7,3), title=\"J-P\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff179dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd74e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ei, X_test_ei, y_train_ei, y_test_ei = train_test_split(X_over_ei, y_over_ei, test_size=0.3, random_state=42)\n",
    "X_train_ns, X_test_ns, y_train_ns, y_test_ns = train_test_split(X_over_ns, y_over_ns, test_size=0.3, random_state=42)\n",
    "X_train_ft, X_test_ft, y_train_ft, y_test_ft = train_test_split(X_over_ft, y_over_ft, test_size=0.3, random_state=42)\n",
    "X_train_jp, X_test_jp, y_train_jp, y_test_jp = train_test_split(X_over_jp, y_over_jp, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd8241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_ei = X_train_ei['cleaned_post']\n",
    "X_train_ns = X_train_ns['cleaned_post']\n",
    "X_train_ft = X_train_ft['cleaned_post']\n",
    "X_train_jp = X_train_jp['cleaned_post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9278a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test_ei = X_test_ei['cleaned_post']\n",
    "X_test_ns = X_test_ns['cleaned_post']\n",
    "X_test_ft = X_test_ft['cleaned_post']\n",
    "X_test_jp = X_test_jp['cleaned_post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db3455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ei.name, y_test_ei.name = \"E-I\", \"E-I\"\n",
    "y_train_ns.name, y_test_ns.name = \"N-S\", \"N-S\"\n",
    "y_train_ft.name, y_test_ft.name = \"F-T\", \"F-T\"\n",
    "y_train_jp.name, y_test_jp.name = \"J-P\", \"J-P\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a92b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_all_train = [y_train_ei, y_train_ns, y_train_ft,  y_train_jp]\n",
    "y_all_test  = [y_test_ei, y_test_ns, y_test_ft, y_test_jp] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c68577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1431dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a122cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(X_train_ei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1bbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_ei = vectorizer.transform(X_train_ei)\n",
    "X_test_ei  = vectorizer.transform(X_test_ei)\n",
    "\n",
    "X_train_ns = vectorizer.transform(X_train_ns)\n",
    "X_test_ns  = vectorizer.transform(X_test_ns)\n",
    "\n",
    "X_train_ft = vectorizer.transform(X_train_ft)\n",
    "X_test_ft  = vectorizer.transform(X_test_ft)\n",
    "\n",
    "X_train_jp = vectorizer.transform(X_train_jp)\n",
    "X_test_jp  = vectorizer.transform(X_test_jp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea121397",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all_train = [X_train_ei, X_train_ns, X_train_ft, X_train_jp]\n",
    "x_all_test  = [X_test_ei, X_test_ns, X_test_ft, X_test_jp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d224636",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = pd.DataFrame(X_test_ei.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tf_idf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b8b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "!pip install xgboost\n",
    "import xgboost\n",
    "import pickle\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models():\n",
    "    nb_clf  = MultinomialNB(alpha=0.01)\n",
    "    svm_clf = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    dt_clf  = DecisionTreeClassifier(max_depth=7)\n",
    "    rf_clf  = RandomForestClassifier(n_estimators=750)\n",
    "    xgb_clf = xgboost.XGBClassifier(use_label_encoder=False)\n",
    "    return {\"NaiveBayes\":nb_clf, \"SVM\":svm_clf, \"DecisionTree\":dt_clf, \"RandomForest\":rf_clf, \"Xgboost\":xgb_clf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f96a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "_metrics = [\"Accuracy\", \"Accuracy\", \"Accuracy\" ,\"Accuracy\", \"Precision\", \"Precision\", \"Precision\",\"Precision\", \"Recall\",\"Recall\",\"Recall\",\"Recall\", \"F1-Score\", \"F1-Score\", \"F1-Score\", \"F1-Score\", \"Roc-Auc Score\", \"Roc-Auc Score\", \"Roc-Auc Score\", \"Roc-Auc Score\"]\n",
    "_types   = [\"E-I\", \"N-S\", \"F-T\", \"J-P\", \"E-I\", \"N-S\", \"F-T\", \"J-P\", \"E-I\", \"N-S\", \"F-T\", \"J-P\",\"E-I\", \"N-S\", \"F-T\", \"J-P\",\"E-I\", \"N-S\", \"F-T\", \"J-P\"]\n",
    "_columns = [\"NaiveBayes\", \"SVM\", \"DecisionTree\", \"RandomForest\", \"Xgboost\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34193cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = create_models()\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99408aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = pd.DataFrame(columns=_columns, index=[_metrics, _types])\n",
    "evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775fbeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_item in models.items():\n",
    "    for X_train, X_test, y_train, y_test in zip(x_all_train, x_all_test, y_all_train, y_all_test):\n",
    "        # Model creation and prediction\n",
    "        MODEL = model_item[1]\n",
    "        print(f\"{model} is training for {y_train.name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        # Performance evaluation metrics\n",
    "        evaluation_df.loc[\"Accuracy\",y_train.name][model_item[0]]      = round(metrics.accuracy_score(y_test, pred), 3)\n",
    "        evaluation_df.loc[\"Precision\",y_train.name][model_item[0]]     = round(metrics.precision_score(y_test, pred), 3)\n",
    "        evaluation_df.loc[\"Recall\",y_train.name][model_item[0]]        = round(metrics.recall_score(y_test, pred), 3)        \n",
    "        evaluation_df.loc[\"F1-Score\",y_train.name][model_item[0]]      = round(metrics.f1_score(y_test, pred), 3)\n",
    "        evaluation_df.loc[\"Roc-Auc Score\",y_train.name][model_item[0]] = round(metrics.roc_auc_score(y_test, pred), 3)        \n",
    "        # Save model\n",
    "        filename = f'{MODEL}{model_item[0]}_{y_test.name}.sav'\n",
    "        print(filename)\n",
    "        pickle.dump(model,open(filename, 'wb'))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6e1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde55d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f29ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
